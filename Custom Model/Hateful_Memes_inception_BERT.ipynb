{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jPFbmksZPQkg",
        "outputId": "03d22863-65d5-4cc2-a0e9-ecf29b0397c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1855k  100 1855k    0     0  1533k      0  0:00:01  0:00:01 --:--:-- 1533k\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./facebookresearch-mmf-v0.3.1-720-g47ee79b.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 9b011606f) to /tmp/pip-install-vvep2b5e/pytorch-lightning_b57d82cb71f149828a3899d6c5bdb140\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-vvep2b5e/pytorch-lightning_b57d82cb71f149828a3899d6c5bdb140\n",
            "\u001b[33m  WARNING: Did not find branch or tag '9b011606f', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 9b011606f\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.4\n",
            "  Downloading matplotlib-3.3.4-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 27.4 MB/s \n",
            "\u001b[?25hCollecting sklearn==0.0\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting pandas-path\n",
            "  Downloading pandas_path-0.3.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting fasttext==0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 255 kB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.8/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Collecting torchaudio<=0.9.0,>=0.6.0\n",
            "  Downloading torchaudio-0.9.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 69.7 MB/s \n",
            "\u001b[?25hCollecting GitPython==3.1.0\n",
            "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
            "\u001b[K     |████████████████████████████████| 450 kB 76.3 MB/s \n",
            "\u001b[?25hCollecting transformers<=4.10.1,>=3.4.0\n",
            "  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 73.4 MB/s \n",
            "\u001b[?25hCollecting pycocotools==2.0.2\n",
            "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
            "Collecting torchvision<=0.10.0,>=0.7.0\n",
            "  Downloading torchvision-0.10.0-cp38-cp38-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting numpy<=1.21.4,>=1.16.6\n",
            "  Downloading numpy-1.21.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.8/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from mmf==1.0.0rc12) (5.4.8)\n",
            "Collecting tqdm<4.50.0,>=4.43.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting torch<=1.9.0,>=1.6.0\n",
            "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 15 kB/s \n",
            "\u001b[?25hCollecting pillow==8.3.1\n",
            "  Downloading Pillow-8.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (from mmf==1.0.0rc12) (4.4.0)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 55.7 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting iopath==0.1.8\n",
            "  Downloading iopath-0.1.8-py3-none-any.whl (19 kB)\n",
            "Collecting omegaconf<=2.1,>=2.0.6\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting ftfy==5.8\n",
            "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting lmdb==0.98\n",
            "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
            "\u001b[K     |████████████████████████████████| 869 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (2.9.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (21.3)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (6.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (2022.11.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 86.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (9.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.6)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.8/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.32)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.0.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (3.8.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (22.1.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 79.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (3.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.38.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.50.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (3.19.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (2.14.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@9b011606f->mmf==1.0.0rc12) (3.2.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (3.8.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 80.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown->mmf==1.0.0rc12) (4.6.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2022.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.10.1,>=3.4.0->mmf==1.0.0rc12) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.1.0)\n",
            "Building wheels for collected packages: mmf, pytorch-lightning, fasttext, ftfy, lmdb, nltk, pycocotools, sklearn, termcolor, antlr4-python3-runtime, sacremoses\n",
            "  Building wheel for mmf (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmf: filename=mmf-1.0.0rc12-cp38-cp38-linux_x86_64.whl size=533090 sha256=b5b3dcd7830bede4dd9ac77fc243c1112d0b34fe8d82390baa4b383963bbcf57\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/5b/16/0d17cddb7914a604182595680952d0891d3f94e1d0d255121a\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.6.0.dev0-py3-none-any.whl size=561940 sha256=dcccddd0ed45fcdd5f9af90cbe50c7465c02d96d4131fb8cbed089b05081dbd7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1gkdxj2j/wheels/d7/1f/91/280edce7e54bc7a8a086e04feebea7993a5074fc143b45b648\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp38-cp38-linux_x86_64.whl size=2502368 sha256=6c0b35ca2dada036a34f7b1f86712373e9f750fe686763442442d4efc2301c58\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/de/57/385baa787dcec2e40c736288c39706cad2222e3c6027476128\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45632 sha256=288d64a68ba1bfd39380b9512120f43984e385ecd12af5cfe8659bac68a870ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/a5/65/684a672b6a26cb8ce3934d155c98d0e23b3dce3d2c0fadae19\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp38-cp38-linux_x86_64.whl size=223506 sha256=73f3a2ef586f726ff6cc4db17571cb7d49acfaeb689f08c763b5de1f4e36e47c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/2a/7f/48049ae692fce03f173a8751185e20c8a449ca0fe8195d7bc7\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=26c4a1f978a23c018eb8061fb689710edd0986e4dbe77776243b279e73d4bf49\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/18/48/8fd6ec11da38406b309470566d6f099c04805d2ec61d7829e7\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp38-cp38-linux_x86_64.whl size=304567 sha256=2862dcebb340b06253b0f9e0f7182691cbde703168e8927e4a403c4b79fecf1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/77/b2/6f38b5bea571cd8f4689f91a7c1ed2eaecb2c2ce17f9945b17\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=7c7143e27c82e1a59658078e19014d5099dbe17ba1458d81ff20929e930945c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=711a9a9b38d485fdb62608300ec92af0f74a9d6c9eed6a5f300d34e6e5fa8650\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=1aee42ac506ea5182d9f570643139442e57febccd57d63c8ac4a279e9ee267a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0cfbee017fd3cee11ba3d6db1794bbc87b015a3f444cc149fc5ce3cd9b13f46a\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built mmf pytorch-lightning fasttext ftfy lmdb nltk pycocotools sklearn termcolor antlr4-python3-runtime sacremoses\n",
            "Installing collected packages: numpy, tqdm, torch, smmap, pillow, xxhash, torchmetrics, tokenizers, sentencepiece, sacremoses, pyDeprecate, pybind11, portalocker, multiprocess, matplotlib, huggingface-hub, gitdb, antlr4-python3-runtime, transformers, torchvision, torchtext, torchaudio, termcolor, sklearn, pytorch-lightning, pycocotools, pandas-path, omegaconf, nltk, lmdb, iopath, GitPython, ftfy, fasttext, datasets, mmf\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.1.1\n",
            "    Uninstalling termcolor-2.1.1:\n",
            "      Successfully uninstalled termcolor-2.1.1\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.6\n",
            "    Uninstalling pycocotools-2.0.6:\n",
            "      Successfully uninstalled pycocotools-2.0.6\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "Successfully installed GitPython-3.1.0 antlr4-python3-runtime-4.8 datasets-1.2.1 fasttext-0.9.1 ftfy-5.8 gitdb-4.0.10 huggingface-hub-0.11.1 iopath-0.1.8 lmdb-0.98 matplotlib-3.3.4 mmf-1.0.0rc12 multiprocess-0.70.14 nltk-3.4.5 numpy-1.21.4 omegaconf-2.1.0 pandas-path-0.3.0 pillow-8.3.1 portalocker-2.6.0 pyDeprecate-0.3.2 pybind11-2.10.1 pycocotools-2.0.2 pytorch-lightning-1.6.0.dev0 sacremoses-0.0.53 sentencepiece-0.1.97 sklearn-0.0 smmap-5.0.0 termcolor-1.1.0 tokenizers-0.10.3 torch-1.9.0 torchaudio-0.9.0 torchmetrics-0.11.0 torchtext-0.5.0 torchvision-0.10.0 tqdm-4.49.0 transformers-4.10.1 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!curl -L -o facebookresearch-mmf-v0.3.1-720-g47ee79b.tar.gz -O \"https://github.com/kartikaykaushik14/HatefulMemes/blob/main/Modified%20Libraries/facebookresearch-mmf-v0.3.1-720-g47ee79b.tar.gz?raw=true\"\n",
        "!pip install facebookresearch-mmf-v0.3.1-720-g47ee79b.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kcc0BXxrkLD",
        "outputId": "84857ada-aa32-4030-da9b-ee1e273a0a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.49.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.4\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ad8U0G9mgZG-LGEgI-X1EmxMDYa6rRPN\n",
            "To: /content/hateful_memes.zip\n",
            "100% 4.23G/4.23G [00:23<00:00, 176MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/resolvers/__init__.py:12: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
            "  warnings.warn(\n",
            "Data folder is ./\n",
            "Zip path is ./hateful_memes.zip\n",
            "Copying ./hateful_memes.zip\n",
            "Unzipping ./hateful_memes.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown --id 1ad8U0G9mgZG-LGEgI-X1EmxMDYa6rRPN\n",
        "!mmf_convert_hm --zip_file ./hateful_memes.zip --password \"password\" --bypass_checksum=1 --mmf_data_folder ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8zU8UbQR8Oa",
        "outputId": "2b3888c8-1a8d-4eeb-8266-78e4f50133a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/hateful_memes/defaults/annotations\n",
            "dev_seen.jsonl    \u001b[0m\u001b[01;34mimg\u001b[0m/             test_unseen.jsonl\n",
            "dev_unseen.jsonl  test_seen.jsonl  train.jsonl\n"
          ]
        }
      ],
      "source": [
        "%cd ./datasets/hateful_memes/defaults/annotations\n",
        "%ls "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_vh7Bw1AXgt",
        "outputId": "b524e417-8934-461b-ac43-04e42b35fe41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.10.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.49.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.12->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "#install transformers(huggingface for BERT)\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9X4cGXLbgiA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tarfile\n",
        "import tempfile\n",
        "import warnings\n",
        "import fasttext\n",
        "import torchvision\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_path  # Path style access for pandas\n",
        "from tqdm import tqdm\n",
        "\n",
        "import transformers as ppb # pytorch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odxH0aoyjmqa"
      },
      "outputs": [],
      "source": [
        "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Uses jsonl data to preprocess and serve \n",
        "    dictionary of multimodal tensors for model input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        image_transform,\n",
        "        text_transform,\n",
        "        balance=False,\n",
        "        dev_limit=None,\n",
        "        random_state=0,\n",
        "    ):\n",
        "        \n",
        "        self.samples_frame = pd.read_json(\n",
        "            data_path, lines=True\n",
        "        )\n",
        "        self.dev_limit = dev_limit\n",
        "        if balance:\n",
        "            neg = self.samples_frame[\n",
        "                self.samples_frame.label.eq(0)\n",
        "            ]\n",
        "            pos = self.samples_frame[\n",
        "                self.samples_frame.label.eq(1)\n",
        "            ]\n",
        "            self.samples_frame = pd.concat(\n",
        "                [\n",
        "                    neg.sample(\n",
        "                        pos.shape[0], \n",
        "                        random_state=random_state\n",
        "                    ), \n",
        "                    pos\n",
        "                ]\n",
        "            )\n",
        "        if self.dev_limit:\n",
        "            if self.samples_frame.shape[0] > self.dev_limit:\n",
        "                self.samples_frame = self.samples_frame.sample(\n",
        "                    dev_limit, random_state=random_state\n",
        "                )\n",
        "        self.samples_frame = self.samples_frame.reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "        self.samples_frame.img = self.samples_frame.apply(\n",
        "            lambda row: (img_dir / row.img), axis=1\n",
        "        )\n",
        "\n",
        "        # https://github.com/drivendataorg/pandas-path\n",
        "        # print(self.samples_frame.img)\n",
        "        # if not self.samples_frame.img.path.exists().all():\n",
        "        #     raise FileNotFoundError\n",
        "        # if not self.samples_frame.img.path.is_file().all():\n",
        "        #     raise TypeError\n",
        "            \n",
        "        self.image_transform = image_transform\n",
        "        self.text_transform = text_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"This method is called when you do len(instance) \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"This method is called when you do instance[key] \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
        "\n",
        "        image = Image.open(\n",
        "            self.samples_frame.loc[idx, \"img\"]\n",
        "        ).convert(\"RGB\")\n",
        "        image = self.image_transform(image)\n",
        "        text = torch.Tensor(\n",
        "#             self.text_transform.wv[self.samples_frame.loc[idx, \"text\"]]\n",
        "# for BERT 2lines commented blow\n",
        "#             self.text_transform.get_sentence_vector(\n",
        "#                 self.samples_frame.loc[idx, \"text\"]\n",
        "#             )\n",
        "            self.text_transform[idx]\n",
        "        ).squeeze()\n",
        "\n",
        "        if \"label\" in self.samples_frame.columns:\n",
        "            label = torch.Tensor(\n",
        "                [self.samples_frame.loc[idx, \"label\"]]\n",
        "            ).long().squeeze()\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image\": image, \n",
        "                \"text\": text, \n",
        "                \"label\": label\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image\": image, \n",
        "                \"text\": text\n",
        "            }\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p818wJtZlpjf"
      },
      "outputs": [],
      "source": [
        "class LanguageAndVisionConcat(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss_fn,\n",
        "        language_module,\n",
        "        vision_module,\n",
        "        language_feature_dim,\n",
        "        vision_feature_dim,\n",
        "        fusion_output_size,\n",
        "        dropout_p,\n",
        "        \n",
        "    ):\n",
        "        super(LanguageAndVisionConcat, self).__init__()\n",
        "        self.language_module = language_module\n",
        "        self.vision_module = vision_module\n",
        "        self.fusion = torch.nn.Linear(\n",
        "            in_features=(language_feature_dim + vision_feature_dim), \n",
        "            out_features=fusion_output_size\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(\n",
        "            in_features=fusion_output_size, \n",
        "            out_features=num_classes\n",
        "        )\n",
        "        self.loss_fn = loss_fn\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "        \n",
        "    def forward(self, text, image, label=None):\n",
        "        text_features = torch.nn.functional.relu(\n",
        "            self.language_module(text)\n",
        "        )\n",
        "        image_features = torch.nn.functional.relu(\n",
        "            self.vision_module(image)\n",
        "        )\n",
        "#         print(\"text shape\")\n",
        "#         print(text_features.shape)\n",
        "#         print(\"img shape\")\n",
        "#         print(image_features.shape)\n",
        "        \n",
        "        combined = torch.cat(\n",
        "            [text_features, image_features], dim=1\n",
        "        )\n",
        "        fused = self.dropout(\n",
        "            torch.nn.functional.relu(\n",
        "            self.fusion(combined)\n",
        "            )\n",
        "        )\n",
        "        logits = self.fc(fused)\n",
        "        pred = torch.nn.functional.softmax(logits)\n",
        "        loss = (\n",
        "            self.loss_fn(pred, label) \n",
        "            if label is not None else label\n",
        "        )\n",
        "        return (pred, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-iolrCpjx0d"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# for the purposes of this post, we'll filter\n",
        "# much of the lovely logging info from our LightningModule\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class HatefulMemesModel(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
        "            # ok, there's one for-loop but it doesn't count\n",
        "            if data_key not in hparams.keys():\n",
        "                raise KeyError(\n",
        "                    f\"{data_key} is a required hparam in this model\"\n",
        "                )\n",
        "        \n",
        "        super(HatefulMemesModel, self).__init__()\n",
        "        for key in hparams.keys():\n",
        "            self.hparams[key]=hparams[key]\n",
        "        \n",
        "        # assign some hparams that get used in multiple places\n",
        "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
        "        self.language_feature_dim = self.hparams.get(\n",
        "            \"language_feature_dim\", 300\n",
        "        )\n",
        "        self.vision_feature_dim = self.hparams.get(\n",
        "            # balance language and vision features by default\n",
        "            \"vision_feature_dim\", self.language_feature_dim\n",
        "        )\n",
        "        self.output_path = Path(\n",
        "            self.hparams.get(\"output_path\", \"model-outputs\")\n",
        "        )\n",
        "        self.output_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        # instantiate transforms, datasets\n",
        "        self.text_transform = self._BERT_text_transform()\n",
        "        self.image_transform = self._build_image_transform()\n",
        "        self.train_dataset = self._build_dataset(\"train_path\")\n",
        "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
        "        \n",
        "        # set up model and training\n",
        "        self.model = self._build_model()\n",
        "        self.trainer_params = self._get_trainer_params()\n",
        "        \n",
        "        #BERT test\n",
        "        #self._BERT_text_transform()\n",
        "        \n",
        "    \n",
        "    ## Required LightningModule Methods (when validating) ##\n",
        "    \n",
        "    def forward(self, text, image, label=None):\n",
        "        return self.model(text, image, label)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        preds, loss = self.forward(\n",
        "            text=batch[\"text\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"label\"]\n",
        "        )\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self.trainer.progress_bar_callback.main_progress_bar.write(\n",
        "            f\"Epoch {self.trainer.current_epoch} training loss={self.trainer.progress_bar_dict['loss']}\")\n",
        "        \n",
        "    def test_step(self, batch, batch_nb):\n",
        "        preds, loss = self.forward(\n",
        "            text=batch[\"text\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"label\"]\n",
        "        )\n",
        "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return {\"test_loss\": loss}\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        self.trainer.progress_bar_callback.main_progress_bar.write(\n",
        "            f\"Epoch {self.trainer.current_epoch} test loss={self.trainer.progress_bar_dict['loss']}\")\n",
        "        \n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        preds, loss = self.eval().forward(\n",
        "            text=batch[\"text\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"label\"]\n",
        "        )\n",
        "        \n",
        "        return {\"batch_val_loss\": loss}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_loss\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "        \n",
        "        self.trainer.progress_bar_callback.main_progress_bar.write(\n",
        "            f\"Epoch {self.trainer.current_epoch} validation loss={avg_loss.item()}\")\n",
        "        \n",
        "        return {\n",
        "            \"val_loss\": avg_loss,\n",
        "            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(), \n",
        "            lr=self.hparams.get(\"lr\", 0.001)\n",
        "        )\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "                \"strict\": False\n",
        "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
        "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
        "          },\n",
        "        }\n",
        "\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset, \n",
        "            shuffle=True, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.dev_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "    \n",
        "    ## Convenience Methods ##\n",
        "    \n",
        "    def fit(self):\n",
        "        self._set_seed(self.hparams.get(\"random_state\", 42))\n",
        "        self.trainer = pl.Trainer(**self.trainer_params)\n",
        "        self.trainer.fit(self)\n",
        "        \n",
        "    def _set_seed(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def _build_text_transform(self):\n",
        "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
        "            ft_path = Path(ft_training_data.name)\n",
        "            with ft_path.open(\"w\") as ft:\n",
        "                training_data = [\n",
        "                    json.loads(line)[\"text\"] + \"/n\" \n",
        "                    for line in open(\n",
        "                        self.hparams.get(\"train_path\")\n",
        "                    ).read().splitlines()\n",
        "                ]\n",
        "                for line in training_data:\n",
        "                    ft.write(line + \"\\n\")\n",
        "                language_transform = fasttext.train_unsupervised(\n",
        "                    str(ft_path),\n",
        "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
        "                    dim=self.embedding_dim\n",
        "                )\n",
        "        return old_language_transform\n",
        "    \n",
        "    def _BERT_text_transform(self):\n",
        "        train_df = pd.read_json(self.hparams.get(\"train_path\"), lines=True)\n",
        "#         train_df = train_df[:20]\n",
        "        model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "        tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        BERT_model = model_class.from_pretrained(pretrained_weights)\n",
        "        tokenized = train_df.text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "        \n",
        "        max_len = 0\n",
        "        for i in tokenized.values:\n",
        "            if len(i) > max_len:\n",
        "                max_len = len(i) # calculates the maximum length among all token seqeuences\n",
        "#         print(\"maximum length is : \", max_len)\n",
        "\n",
        "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values]) # does the padding.\n",
        "    \n",
        "#         for i in range(10):\n",
        "#             print(padded[i])     \n",
        "#         print(padded.shape)\n",
        "        \n",
        "        attention_mask = np.where(padded != 0, 1, 0)\n",
        "        input_ids = torch.tensor(padded)  # constructs a tensor by copying `data`\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        # We now create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
        "        with torch.no_grad():\n",
        "            last_hidden_states = BERT_model(input_ids, attention_mask=attention_mask)        \n",
        "        print(last_hidden_states[0].shape)\n",
        "        language_transform = last_hidden_states[0][:,0,:].numpy()\n",
        "        return language_transform\n",
        "        \n",
        "        \n",
        "    def _build_image_transform(self):\n",
        "        image_dim = self.hparams.get(\"image_dim\", 224)\n",
        "        image_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(\n",
        "                    size=(image_dim, image_dim)\n",
        "                ),        \n",
        "                torchvision.transforms.ToTensor(),\n",
        "                # all torchvision models expect the same\n",
        "                # normalization mean and std\n",
        "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406), \n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        return image_transform\n",
        "\n",
        "    def _build_dataset(self, dataset_key):\n",
        "        return HatefulMemesDataset(\n",
        "            data_path=self.hparams.get(dataset_key, dataset_key),\n",
        "            img_dir=self.hparams.get(\"img_dir\"),\n",
        "            image_transform=self.image_transform,\n",
        "            text_transform=self.text_transform,\n",
        "            # limit training samples only\n",
        "            dev_limit=(\n",
        "                self.hparams.get(\"dev_limit\", None) \n",
        "                if \"train\" in str(dataset_key) else None\n",
        "            ),\n",
        "            balance=False if \"train\" in str(dataset_key) else False,\n",
        "        )\n",
        "    \n",
        "    def _build_model(self):\n",
        "        # we're going to pass the outputs of our text\n",
        "        # transform through an additional trainable layer\n",
        "        # rather than fine-tuning the transform\n",
        "        language_module = torch.nn.Linear(\n",
        "                in_features=self.embedding_dim,\n",
        "                out_features=self.language_feature_dim\n",
        "        )\n",
        "        \n",
        "        # easiest way to get features rather than\n",
        "        # classification is to overwrite last layer\n",
        "        # with an identity transformation, we'll reduce\n",
        "        # dimension using a Linear layer, resnet is 2048 out\n",
        "        vision_module = torchvision.models.inception_v3(\n",
        "            pretrained=True, aux_logits = False\n",
        "        )\n",
        "        \n",
        "#         vision_module.AuxLogits.fc = torch.nn.Linear(768, self.vision_feature_dim)\n",
        "        vision_module.fc = torch.nn.Linear(2048, self.vision_feature_dim)\n",
        "        \n",
        "\n",
        "        return LanguageAndVisionConcat(\n",
        "            num_classes=self.hparams.get(\"num_classes\", 2),\n",
        "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
        "            language_module=language_module,\n",
        "            vision_module=vision_module,\n",
        "            language_feature_dim=self.language_feature_dim,\n",
        "            vision_feature_dim=self.vision_feature_dim,\n",
        "            fusion_output_size=self.hparams.get(\n",
        "                \"fusion_output_size\", 512\n",
        "            ),\n",
        "            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n",
        "        )\n",
        "    \n",
        "    def _get_trainer_params(self):\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            # filepath=self.output_path,\n",
        "            dirpath=self.output_path,\n",
        "            monitor=self.hparams.get(\n",
        "                \"checkpoint_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            mode=self.hparams.get(\n",
        "                \"checkpoint_monitor_mode\", \"min\"\n",
        "            ),\n",
        "            verbose=self.hparams.get(\"verbose\", True)\n",
        "        )\n",
        "\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor=self.hparams.get(\n",
        "                \"early_stop_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            min_delta=self.hparams.get(\n",
        "                \"early_stop_min_delta\", 0.001\n",
        "            ),\n",
        "            patience=self.hparams.get(\n",
        "                \"early_stop_patience\", 3\n",
        "            ),\n",
        "            verbose=self.hparams.get(\"verbose\", True),\n",
        "        )\n",
        "\n",
        "        trainer_params = {\n",
        "            # \"checkpoint_callback\": checkpoint_callback,\n",
        "            # \"callbacks\": early_stop_callback,\n",
        "            \"default_root_dir\": self.output_path,\n",
        "            \"accumulate_grad_batches\": self.hparams.get(\n",
        "                \"accumulate_grad_batches\", 1\n",
        "            ),\n",
        "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
        "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
        "            \"gradient_clip_val\": self.hparams.get(\n",
        "                \"gradient_clip_value\", 1\n",
        "            ),\n",
        "        }\n",
        "        return trainer_params\n",
        "            \n",
        "    @torch.no_grad()\n",
        "    def make_submission_frame(self, test_path):\n",
        "        test_dataset = self._build_dataset(test_path)\n",
        "        submission_frame = pd.DataFrame(\n",
        "            index=test_dataset.samples_frame.id,\n",
        "            columns=[\"proba\", \"label\"]\n",
        "        )\n",
        "        test_dataloader = torch.utils.data.DataLoader(\n",
        "            test_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16))\n",
        "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "            preds, _ = self.model.eval().to(\"cpu\")(\n",
        "                batch[\"text\"], batch[\"image\"]\n",
        "            )\n",
        "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
        "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
        "        submission_frame.proba = submission_frame.proba.astype(float)\n",
        "        submission_frame.label = submission_frame.label.astype(int)\n",
        "        return submission_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "703jdhVEnKdh",
        "outputId": "ebc74ec0-48ca-4689-c14c-ef9c58019771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/hateful_memes/defaults/annotations\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path.cwd().parent / \"annotations\"\n",
        "print(data_dir)\n",
        "img_tar_path = data_dir / \"img.tar.gz\"\n",
        "train_path = data_dir / \"train.jsonl\"\n",
        "dev_path = data_dir / \"test_unseen.jsonl\"\n",
        "test_path = data_dir / \"test_unseen.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j17Ha2PCLLJz",
        "outputId": "55a62053-a105-432d-c691-4ac45847ea2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5481\n",
              "1    3019\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_samples_frame = pd.read_json(train_path, lines=True)\n",
        "train_samples_frame.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "HcXoTixej55G",
        "outputId": "2476c53d-f7fc-43a5-d85b-2fc6bf2284e9",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-adb7d314ddc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Required hparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m\"train_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"dev_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"img_dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
          ]
        }
      ],
      "source": [
        "hparams = {\n",
        "    \n",
        "    # Required hparams\n",
        "    \"train_path\": train_path,\n",
        "    \"dev_path\": dev_path,\n",
        "    \"img_dir\": data_dir,\n",
        "    \n",
        "    # Optional hparams\n",
        "    \"embedding_dim\": 768,\n",
        "    \"language_feature_dim\": 768,\n",
        "    \"vision_feature_dim\": 300,\n",
        "    \"fusion_output_size\": 256,\n",
        "    \"output_path\": \"model-outputs\",\n",
        "    \"dev_limit\": None,\n",
        "    \"lr\": 0.00005,\n",
        "    \"max_epochs\": 5,\n",
        "    \"n_gpu\": 1,\n",
        "    \"batch_size\": 4,\n",
        "    # allows us to \"simulate\" having larger batches \n",
        "    \"accumulate_grad_batches\": 16,\n",
        "    \"early_stop_patience\": 3,\n",
        "}\n",
        "\n",
        "hateful_memes_model = HatefulMemesModel(hparams=hparams)\n",
        "\n",
        "#this just test the BERT frame\n",
        "#hateful_memes_model._BERT_text_transform()\n",
        "\n",
        "#comment below for testing\n",
        "hateful_memes_model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FKAAAizeoDF"
      },
      "outputs": [],
      "source": [
        "checkpoints = list(Path(\"model-outputs/lightning_logs/version_4/checkpoints\").glob(\"*.ckpt\"))\n",
        "# assert len(checkpoints) == 1\n",
        "\n",
        "submission = hateful_memes_model.make_submission_frame(test_path)\n",
        "submission.head()\n",
        "\n",
        "submission.to_csv((\"model-outputs/submission.csv\"), index=True)\n",
        "\n",
        "columns = [\"id\", \"label\"]\n",
        "df = pd.read_csv(\"./model-outputs/submission.csv\", usecols=columns)\n",
        "\n",
        "import json\n",
        "\n",
        "data = pd.read_json(test_path, lines=True)\n",
        "\n",
        "accuracy = 0\n",
        "row,col = df.shape\n",
        "for csv_row in range(row):\n",
        "  if(df.label[csv_row] == data.label[csv_row]):\n",
        "    accuracy+=1\n",
        "\n",
        "# print(str((accuracy/1000) * 100) + \"%\")\n",
        "print(\"%.2f%%\"%((accuracy/2000) * 100) )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}